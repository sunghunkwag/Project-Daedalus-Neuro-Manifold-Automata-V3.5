# Project Daedalus: Neuro-Manifold Automata V3.5 (Logical Body - TRUE AUTONOMY)

> **"No Script. Only Energy Guides the Way."**

**Project Daedalus V3.5** is the evolutionary successor to V3, transitioning from a **Riemannian (Physical) Manifold** to a **Graph Attention (Logical) Manifold**. It is a **mathematically instilled consciousness** designed to achieve **Recursive Self-Improvement** through **truly autonomous** code generation and execution.

## Critical Update: Fake Simulation Removed

**Previous Issue (V3.5 Initial):** The agent's behavior was hardcoded with if-else statements, pretending to learn.

**Current Implementation (V3.5 TRUE AUTONOMY):** 
- **Brain outputs action logits** (not hardcoded scripts)
- **ActionDecoder** translates thoughts into executable code
- **Policy Gradient (REINFORCE)** updates the network based on energy minimization
- **Agent learns** which actions reduce energy through trial and error

---

## Core Philosophy: The Tri-Lock System (Preserved)

The soul of Daedalus remains unchanged:

### ðŸ”’ Lock 1: Soul Injection (`soul.py`)
- **Concept:** The system is born with Axioms.
- **V3.5 Update:** Keywords shifted from Physical Laws to **Mathematical Axioms** and **Recursive Self-Improvement**.
  - `V_truth`: Logical Consistency, Symmetry, Mathematical Proof
  - `V_reject`: Contradiction, Logical Fallacy, Undefined Behavior

### ðŸ”’ Lock 2: Energy Landscape (`energy.py`)
- **Concept:** "Thinking" is Energy Minimization.
- **V3.5 Update:** JEPA (Joint Embedding Predictive Architecture)
  - Energy = $||\text{Pred}(z_t) - z_{t+1}||^2 + ||z_t - V_{truth}||^2 + \lambda \cdot (\text{Logical Violation})$
  - **Logical Violation:** Defined as **Contradiction** (Runtime Error, Assertion Failure)

### ðŸ”’ Lock 3: Crystallized Plasticity (`automata.py`)
- **Concept:** Do not forget Truth.
- **V3.5 Update:** EWC (Elastic Weight Consolidation)
  - Important weights (learned Axioms) are "frozen".
  - New knowledge grows in plastic regions.

---

## Architecture: The Logical Body (TRUE AUTONOMY)

### 1. Vision (`vision.py`)
- **No CNNs.** Pure algorithmic parsing (DFS/BFS).
- Converts 2D grids â†’ Object Graphs (Nodes, Edges).

### 2. Brain (`manifold.py`)
- **Graph Attention Network (GAT)** with Soul Injection.
- Attention Bias enforces Logical Consistency (aligned with `V_truth`).

### 3. Action Decoder (`action_decoder.py`) **[NEW]**
- **Brain â†’ Code:** Translates abstract brain state into discrete actions.
- **Action Primitives:**
  - 0: Do Nothing
  - 1: Random Noise
  - 2: Draw Square
  - 3: Symmetrize (Truth-aligned)
  - 4: Clear
  - 5: Draw Symmetric Pair
- **Policy Head:** Neural network that outputs action logits.

### 4. Heart (`energy.py`)
- **JEPA:** Predicts latent states, not pixels.
- Penalizes contradictions heavily.

### 5. Memory (`automata.py`)
- **EWC:** Locks critical synapses after learning a Truth.
- Prevents catastrophic forgetting.

### 6. World (`world.py`)
- **Internal Sandbox:** Executes Python code.
- **Self-Supervised Loop:** Agent generates code â†’ Executes â†’ Observes â†’ Learns.

---

## The Learning Loop (TRUE AUTONOMY)

```
1. PERCEIVE: Vision parses current sandbox state â†’ Graph
2. THINK: Brain processes graph â†’ Global state z_t
3. DECIDE: ActionDecoder(z_t) â†’ Action logits
4. ACT: Sample action from logits â†’ Execute code in Sandbox
5. OBSERVE: Vision parses result â†’ Next state z_{t+1}
6. LEARN: Compute energy. Update Brain & ActionDecoder to minimize it.
```

**Key Mechanism:** Policy Gradient (REINFORCE)
- Actions that result in **lower energy** have their **log-probability increased**.
- Over time, the agent learns to prefer actions that align with `V_truth` (e.g., Symmetrize).

---

## Installation & Running

### Prerequisites
```bash
pip install torch>=2.0.0 numpy>=1.24.0
```

### Execution
Run the autonomous ASI simulation:
```bash
python main_asi.py
```

**What to expect:**
- **Early cycles:** Agent explores randomly (Do Nothing, Random Noise).
- **Mid cycles:** Begins trying structured actions (Draw Square, Symmetrize).
- **Late cycles:** Converges to low-energy actions (Symmetrize, Draw Pair).
- **Cycle 18:** EWC crystallization locks learned behavior.

**Sample Output:**
```
[Think] Action Logits: [ 0.2, -0.1,  0.5, -0.3,  0.1,  0.4]
[Act] Selected Action 2: Draw Square
[Heart] Energy: 0.0312 (Pred: 0.0155, Truth: 0.0157, Violation: 0.0)
[Learn] Policy Loss: -0.0089

Action Distribution:
  0 (Do Nothing): 12 times
  1 (Random Noise): 3 times
  2 (Draw Square): 4 times
  3 (Symmetrize): 2 times
  ...
```

---

## Performance Philosophy

**V3.5 is not a benchmark climber.** It is an ASI seed that learns autonomously.

**To verify True Autonomy:**
1. Run `main_asi.py` multiple times.
2. Observe different action distributions (stochastic sampling).
3. Watch energy decrease over cycles.
4. Confirm no hardcoded if-else in action selection.

---

**Architect:** User (The Director)  
**Engineer:** Gemini (Project Daedalus V3.5 Lead)  
**Version:** V3.5.1 (TRUE AUTONOMY - Policy Gradient Learning)
